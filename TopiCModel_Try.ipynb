{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
      "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
      "order to load all the package's dependencies. You can do this by selecting the\n",
      "'Restart kernel' or 'Restart runtime' option.\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.cli.download import download\n",
    "download(model=\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en import English\n",
    "parser = English()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    lda_tokens = []\n",
    "    tokens = parser(text)\n",
    "    for token in tokens:\n",
    "        if token.orth_.isspace():\n",
    "            continue\n",
    "        elif token.like_url:\n",
    "            lda_tokens.append('URL')\n",
    "        elif token.orth_.startswith('@'):\n",
    "            lda_tokens.append('SCREEN_NAME')\n",
    "        else:\n",
    "            lda_tokens.append(token.lower_)\n",
    "    return lda_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vaibh\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe: No module named spacy\n"
     ]
    }
   ],
   "source": [
    "!python3 -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\vaibh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "def get_lemma(word):\n",
    "    lemma = wn.morphy(word)\n",
    "    if lemma is None:\n",
    "        return word\n",
    "    else:\n",
    "        return lemma\n",
    "    \n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "def get_lemma2(word):\n",
    "    return WordNetLemmatizer().lemmatize(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\vaibh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "en_stop = set(nltk.corpus.stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_text_for_lda(text):\n",
    "    tokens = tokenize(text)\n",
    "    tokens = [token for token in tokens if len(token) > 4]\n",
    "    tokens = [token for token in tokens if token not in en_stop]\n",
    "    tokens = [get_lemma(token) for token in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['anthropocentric', 'system']\n",
      "['friend', 'close', 'incorporate', 'trust', 'social', 'network', 'base', 'sybil', 'defense']\n",
      "['automate', 'semantic', 'services', 'orchestration', 'concept', 'covering']\n",
      "['energy', 'function', 'base', 'neural', 'network', 'transient', 'stability', 'enhancement', 'network', 'preserve', 'power', 'system']\n",
      "['bound', 'power', 'function', 'wireless', 'network']\n",
      "['hyperqueries', 'dynamic', 'distribute', 'query', 'processing', 'internet']\n",
      "['memory', 'efficient', 'layer', 'decoder', 'design', 'early', 'termination', 'code']\n",
      "['single', 'miller', 'compensation', 'using', 'invert', 'current', 'buffer', 'multi', 'stage', 'amplifier']\n",
      "['video', 'suggestion', 'discovery', 'youtube', 'taking', 'random', 'walk', 'graph']\n",
      "['factor', 'formatting', 'perceptional', 'impression', 'cyber', 'space', 'cross', 'cultural', 'study', 'korean', 'american', 'user']\n",
      "['tunable', 'aware', 'network', 'survivability']\n",
      "['adaptive', 'neuron', 'circuit', 'signal', 'compression']\n",
      "['step', 'hybrid', 'simulation', 'large', 'scale', 'network']\n",
      "['advance', 'colluder', 'detection', 'technique', 'osift', 'base', 'hiding', 'code']\n",
      "['sndocrank', 'document', 'ranking', 'base', 'social', 'network']\n",
      "['collaborative', 'indian', 'business', 'cluster']\n",
      "['consensus', 'network', 'multi', 'agent', 'system', 'model', 'predictive', 'control', 'horizon']\n",
      "['biomimetic', 'synapse']\n",
      "['double', 'bubble', 'trouble', 'discrete', 'circulation', 'preserve', 'vortex', 'sheet', 'film', 'foam']\n",
      "['optimization', 'system', 'algebraic', 'equation', 'evaluate', 'datalog', 'query']\n",
      "['feasibility', 'study', 'base', 'equalizer', 'optical', 'fiber', 'receiver']\n",
      "['towards', 'second', 'third', 'generation', 'base', 'multimedia']\n",
      "['novel', 'truncate', 'square', 'linear', 'compensation', 'function']\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "text_data = []\n",
    "with open('C:\\\\Users\\\\vaibh\\\\OneDrive\\\\Documents\\\\dataset.csv') as f:\n",
    "    for line in f:\n",
    "        tokens = prepare_text_for_lda(line)\n",
    "        if random.random() > .99:\n",
    "            print(tokens)\n",
    "            text_data.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "\n",
    "# Create a dictionary and corpus for topic modeling with Gensim\n",
    "dictionary = corpora.Dictionary(text_data)\n",
    "corpus = [dictionary.doc2bow(doc) for doc in text_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.062*\"network\" + 0.038*\"base\" + 0.038*\"system\" + 0.026*\"function\"')\n",
      "(1, '0.022*\"preserve\" + 0.022*\"foam\" + 0.022*\"trouble\" + 0.022*\"circulation\"')\n",
      "(2, '0.034*\"code\" + 0.019*\"multi\" + 0.019*\"compensation\" + 0.019*\"amplifier\"')\n",
      "(3, '0.035*\"base\" + 0.019*\"social\" + 0.019*\"query\" + 0.019*\"close\"')\n",
      "(4, '0.008*\"anthropocentric\" + 0.008*\"network\" + 0.008*\"synapse\" + 0.008*\"biomimetic\"')\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "NUM_TOPICS = 5\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics = NUM_TOPICS, id2word=dictionary, passes=15)\n",
    "# ldamodel.save('model5.gensim')\n",
    "topics = ldamodel.print_topics(num_words=4)\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1 belongs to topic: [(0, 0.7694471), (1, 0.115524076), (2, 0.1150288)]\n",
      "Document 2 belongs to topic: [(0, 0.03384763), (1, 0.9317049), (2, 0.034447525)]\n",
      "Document 3 belongs to topic: [(0, 0.90417093), (1, 0.047929566), (2, 0.047899533)]\n",
      "Document 4 belongs to topic: [(0, 0.027547581), (1, 0.030740645), (2, 0.9417118)]\n",
      "Document 5 belongs to topic: [(0, 0.057945102), (1, 0.8821757), (2, 0.059879184)]\n",
      "Document 6 belongs to topic: [(0, 0.049193848), (1, 0.9028901), (2, 0.04791604)]\n",
      "Document 7 belongs to topic: [(0, 0.9249249), (1, 0.037276186), (2, 0.037798904)]\n",
      "Document 8 belongs to topic: [(0, 0.030954663), (1, 0.9385659), (2, 0.03047947)]\n",
      "Document 9 belongs to topic: [(0, 0.037336), (1, 0.03731975), (2, 0.9253443)]\n",
      "Document 10 belongs to topic: [(0, 0.025836233), (1, 0.025826203), (2, 0.9483376)]\n",
      "Document 11 belongs to topic: [(0, 0.06744495), (1, 0.8647494), (2, 0.06780567)]\n",
      "Document 12 belongs to topic: [(0, 0.88819534), (1, 0.055919945), (2, 0.055884734)]\n",
      "Document 13 belongs to topic: [(0, 0.889966), (1, 0.059822693), (2, 0.050211336)]\n",
      "Document 14 belongs to topic: [(0, 0.038335696), (1, 0.03776288), (2, 0.92390144)]\n",
      "Document 15 belongs to topic: [(0, 0.048564665), (1, 0.90143925), (2, 0.049996067)]\n",
      "Document 16 belongs to topic: [(0, 0.067216516), (1, 0.06718507), (2, 0.8655984)]\n",
      "Document 17 belongs to topic: [(0, 0.034896873), (1, 0.93095666), (2, 0.03414649)]\n",
      "Document 18 belongs to topic: [(0, 0.11197628), (1, 0.7761771), (2, 0.11184659)]\n",
      "Document 19 belongs to topic: [(0, 0.030534139), (1, 0.030520122), (2, 0.9389458)]\n",
      "Document 20 belongs to topic: [(0, 0.91466117), (1, 0.043121405), (2, 0.04221743)]\n",
      "Document 21 belongs to topic: [(0, 0.042219013), (1, 0.042537436), (2, 0.91524357)]\n",
      "Document 22 belongs to topic: [(0, 0.8975426), (1, 0.050532967), (2, 0.051924426)]\n",
      "Document 23 belongs to topic: [(0, 0.9008752), (1, 0.050267026), (2, 0.048857737)]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.ldamodel import LdaModel\n",
    "from pprint import pprint\n",
    "\n",
    "# Build the LDA (Latent Dirichlet Allocation) model\n",
    "lda_model = LdaModel(corpus, num_topics=3, id2word=dictionary, passes=15)\n",
    "\n",
    "for i, doc in enumerate(text_data):\n",
    "    bow = dictionary.doc2bow(doc)\n",
    "    print(f\"Document {i+1} belongs to topic: {lda_model.get_document_topics(bow)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
